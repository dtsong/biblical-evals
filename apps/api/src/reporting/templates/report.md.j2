# {{ evaluation.name }} â€” Evaluation Report

**Perspective:** {{ evaluation.perspective }}
**Review Mode:** {{ evaluation.review_mode }}
**Models Evaluated:** {{ evaluation.model_list | join(', ') }}
**Total Responses:** {{ total_responses }}
**Total Scores:** {{ total_scores }}
**Reviewers:** {{ reviewer_count }}

---

## Model Rankings

| Rank | Model | Overall Score |
|------|-------|--------------|
{% for entry in rankings -%}
| {{ entry.rank }} | {{ entry.model }} | {{ entry.overall_score }} / 5.0 |
{% endfor %}

## Per-Model Dimension Scores

{% for model, dims in model_averages.items() -%}
### {{ model }} (Overall: {{ model_overall[model] }})

| Dimension | Average Score |
|-----------|--------------|
{% for dim, score in dims.items() -%}
| {{ dim }} | {{ score }} / 5.0 |
{% endfor %}
{% if strengths_weaknesses[model] is defined -%}
**Strengths:** {{ strengths_weaknesses[model].strengths | join(', ') }}
**Weaknesses:** {{ strengths_weaknesses[model].weaknesses | join(', ') }}
{% endif %}

{% endfor %}

## Head-to-Head Comparisons

{% for model_a, comparisons in head_to_head.items() -%}
{% for model_b, dims in comparisons.items() -%}
### {{ model_a }} vs {{ model_b }}

| Dimension | Difference (+ favors {{ model_a }}) |
|-----------|--------------------------------------|
{% for dim, diff in dims.items() -%}
| {{ dim }} | {{ "+" if diff > 0 }}{{ diff }} |
{% endfor %}

{% endfor -%}
{% endfor %}

---

*Report generated by Biblical Evals Framework*
